---
title: "432 Class 07"
author: "<https://thomaselove.github.io/432-2023/>"
date: "2023-02-07"
date-format: iso
format: 
  beamer:
    theme: Madrid
    colortheme: lily
    fig-align: center
---

## Today's Agenda

- A First Example: Space Shuttle O-Rings
- Predicting a Binary outcome using a single predictor
    - using a linear probability model
    - using logistic regression and `glm`
    - using logistic regression and `lrm`
    
See Chapters 19-21 in our Course Notes for more on these models.

## Today's R Setup

```{r}
#| echo: true
#| message: false
knitr::opts_chunk$set(comment = NA)

library(faraway) # data source
library(broom)
library(knitr)
library(patchwork)
library(rms)
library(tidyverse)

theme_set(theme_bw()) 
```

## Challenger Space Shuttle Data

The US space shuttle Challenger exploded on 1986-01-28. An investigation ensued into the reliability of the shuttle's propulsion system. The explosion was eventually traced to the failure of one of the three field joints on one of the two solid booster rockets. Each of these six field joints includes two O-rings which can fail.

The discussion among engineers and managers raised concern that the probability of failure of the O-rings depended on the temperature at launch, which was forecast to be 31 degrees F. There are strong engineering reasons based on the composition of O-rings to support the judgment that failure probability may rise monotonically as temperature drops. 

We have data on 23 space shuttle flights that preceded *Challenger* on primary o-ring erosion and/or blowby and on the temperature in degrees Fahrenheit. No previous liftoff temperature was under 53 degrees F.

## The "O-rings" data

```{r}
#| echo: true
orings1 <- faraway::orings |>
    tibble() |>
    mutate(burst = case_when( damage > 0 ~ 1,
                              TRUE ~ 0))

orings1 |> summary()
```

- `damage` = number of damage incidents out of 6 possible
- we set `burst` = 1 if `damage` > 0

## Code to plot `burst` and `temp` in our usual way...

```{r}
#| echo: true
#| eval: false
ggplot(orings1, aes(x = factor(burst), y = temp)) +
    geom_violin() + 
    geom_boxplot(aes(fill = factor(burst)), width = 0.3) +
    guides(fill = "none") + 
    labs(title = "Are bursts more common at low temperatures?",
         subtitle = "23 prior space shuttle launches",
         x = "Was there a burst? (1 = yes, 0 = no)", 
         y = "Launch Temp (F)")
```

## Plotted Association of `burst` and `temp`

```{r}
ggplot(orings1, aes(x = factor(burst), y = temp)) +
    geom_violin() + 
    geom_boxplot(aes(fill = factor(burst)), width = 0.3) +
    guides(fill = "none") + 
    labs(title = "Are bursts more common at low temperatures?",
         subtitle = "23 prior space shuttle launches",
         x = "Was there a burst? (1 = yes, 0 = no)", 
         y = "Launch Temp (F)")
```

## What if we want to predict Prob(burst) using temp?

We want to treat the binary variable `burst` as the outcome, and `temp` as the predictor...

```{r}
#| echo: true
#| eval: false
ggplot(orings1, aes(x = temp, y = burst)) +
    geom_point(col = "navy", alpha = 0.3) +
    labs(title = "Are bursts more common at low temperatures",
         subtitle = "23 prior space shuttle launches",
         y = "Was there a burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")
```

## Plot of Prob(burst) by temperature at launch

```{r}
ggplot(orings1, aes(x = temp, y = burst)) +
    geom_point(col = "navy", alpha = 0.3) +
    labs(title = "Are bursts more common at low temperatures",
         subtitle = "23 prior space shuttle launches",
         y = "Was there a burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")
```

# A Linear Probability Model, fit with `lm()`

## Fit a linear model to predict Prob(burst)?

```{r}
#| echo: true
mod1 <- lm(burst ~ temp, data = orings1)

tidy(mod1, conf.int = T) |> kable(digits = 3)
```

- This is a **linear probability model**.

$$
\operatorname{\widehat{burst}} = 2.905 - 0.037(\operatorname{temp})
$$

## Add linear probability model to our plot?

```{r}
ggplot(orings1, aes(x = temp, y = burst)) +
    geom_point(col = "navy", alpha = 0.3) +
    geom_smooth(method = "lm", se = F, col = "red",
                formula = y ~ x) +
    labs(title = "Bursts more common at lower temperatures",
         subtitle = "23 prior space shuttle launches",
         y = "Was there a burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")
```

- It would help if we could see the individual launches...

## Add vertical jitter and our `mod1` model?

```{r}
#| echo: true
#| eval: false
ggplot(orings1, aes(x = temp, y = burst)) +
    geom_jitter(height = 0.1) +
    geom_smooth(method = "lm", se = F, col = "red",
                formula = y ~ x) +
    labs(title = "Bursts more common at lower temperatures",
         subtitle = "23 prior space shuttle launches",
         y = "Was there a burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")
```

## Resulting plot with points jittered and linear model

```{r}
ggplot(orings1, aes(x = temp, y = burst)) +
    geom_jitter(height = 0.1) +
    geom_smooth(method = "lm", se = F, col = "red",
                formula = y ~ x) +
    labs(title = "Bursts more common at lower temperatures",
         subtitle = "23 prior space shuttle launches",
         y = "Was there a burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")
```

- What's wrong with this picture?

## Making Predictions with `mod1`

```{r}
#| echo: true
mod1$coefficients
```

- What does `mod1` predict for the probability of a burst if the temperature at launch is 70 degrees F?

```{r}
#| echo: true
predict(mod1, newdata = tibble(temp = 70))
```

- What if the temperature was actually 60 degrees F?

## Making Several Predictions with `mod1`

Let's use our linear probability model `mod1` to predict the probability of a burst at some other temperatures...

```{r}
#| echo: true
newtemps <- tibble(temp = c(80, 70, 60, 50, 31))

augment(mod1, newdata = newtemps)
```

- Uh, oh.

## Can we build residual plots?

```{r}
#| echo: true
#| eval: false
par(mfrow = c(2,2)); plot(mod1); par(mfrow = c(1,1))
```

See next slide for results...

## Residual Plots for `mod1`?

```{r}
#| fig-height: 5.5
par(mfrow = c(2,2)); plot(mod1); par(mfrow = c(1,1))
```

- Uh, oh.

## Models to predict a Binary Outcome

Our outcome takes on two values (zero or one) and we then model the probability of a "one" response given a linear function of predictors.

Idea 1: Use a *linear probability model*

- Main problem: predicted probabilities that are less than 0 and/or greater than 1
- Also, how can we assume Normally distributed residuals when outcomes are 1 or 0?

Idea 2: Build a *non-linear* regression approach

- Most common approach: logistic regression, part of the class of *generalized* linear models

# A Logistic Regression Model, fit with `glm()`

## The Logit Link and Logistic Function

The function we use in logistic regression is called the **logit link**.

$$
logit(\pi) = log\left( \frac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k
$$

The inverse of the logit function is called the **logistic function**. If logit($\pi$) = $\eta$, then $\pi = \frac{exp(\eta)}{1 + exp(\eta)}$. 

- The logistic function $\frac{e^x}{1 + e^x}$ takes any value $x$ in the real numbers and returns a value between 0 and 1.

## The Logistic Function $y = \frac{e^x}{1 + e^x}$

```{r, echo = FALSE, fig.height = 5}
set.seed(43201)
temp <- tibble(
    x = runif(200, min = -5, max = 5),
    y = exp(x) / (1 + exp(x)))

ggplot(temp, aes(x = x, y = y)) + 
    geom_line()
```

## The logit or log odds

We usually focus on the **logit** in statistical work, which is the inverse of the logistic function.

- If we have a probability $\pi < 0.5$, then $logit(\pi) < 0$.
- If our probability $\pi > 0.5$, then $logit(\pi) > 0$.
- Finally, if $\pi = 0.5$, then $logit(\pi) = 0$.

### Why is this helpful?

- log(odds(Y = 1)) or logit(Y = 1) covers all real numbers.
- Prob(Y = 1) is restricted to [0, 1].

## Predicting Pr(event) or Pr(no event)

- Can we flip the story?

```{r, echo = FALSE, fig.height = 5}
set.seed(43201)
temp <- tibble(
    x = runif(200, min = -5, max = 5),
    y = exp(x) / (1 + exp(x)),
    y2 = 1 - y)

p1 <- ggplot(temp, aes(x = x, y = y)) + 
    geom_line() + 
    labs(y = "Prob(event occurs)")
p2 <- ggplot(temp, aes(x = x, y = y2)) + 
    geom_line() +
    labs(y = "Prob(no event)")

p1 + p2
```

## Returning to the prediction of Prob(burst)

We'll use the `glm` function in R, specifying a logistic regression model.

- Instead of predicting $Pr(burst)$, we're predicting $log(odds(burst))$ or $logit(burst)$.

```{r}
#| echo: true
mod2 <- glm(burst ~ temp, data = orings1,
            family = binomial(link = "logit"))

tidy(mod2, conf.int = TRUE) |> 
  select(term, estimate, std.error, conf.low, conf.high) |>
  kable(digits = c(0,4,3,3,3))
```

## Our model `mod2`

$$
\log\left[ \frac { \widehat{P( \operatorname{burst} = \operatorname{1} )} }{ 1 - \widehat{P( \operatorname{burst} = \operatorname{1} )} } \right] = 15.0429 - 0.2322(\operatorname{temp})
$$

- For a temperature of 70 F at launch, what is the prediction?

## Let's look at the results

- For a temperature of 70 F at launch, what is the prediction?

log(odds(burst)) = 15.0429 - 0.2322 (70) = -1.211

- Exponentiate to get the odds, on our way to estimating the probability.

odds(burst) = exp(-1.211) = 0.2979

- so, we can estimate the probability by

$$
Pr(burst) = \frac{0.2979}{(0.2979+1)} = 0.230.
$$

## Prediction from `mod2` for temp = 60

What is the predicted probability of a burst if the temperature is 60 degrees?


- log(odds(burst)) = 15.0429 - 0.2322 (60) = 1.1109

- odds(burst) = exp(1.1109) = 3.0371

- Pr(burst) = 3.0371 / (3.0371 + 1) = 0.752


## Will `augment` do this, as well?

```{r}
#| echo: true
temps <- tibble(temp = c(60,70))

augment(mod2, newdata = temps, type.predict = "link")
augment(mod2, newdata = temps, type.predict = "response")
```

## Plotting the Logistic Regression Model

Use the `augment` function to get the fitted probabilities into the original data, then plot.

```{r}
#| echo: true
#| eval: false

mod2_aug <- augment(mod2, type.predict = "response")

ggplot(mod2_aug, aes(x = temp, y = burst)) +
  geom_point(alpha = 0.4) +
  geom_line(aes(x = temp, y = .fitted), 
            col = "purple", size = 1.5) +
  labs(title = "Fitted Logistic mod2 for Pr(burst)")
```

- Results on next slide

## Plotting Model `m2`

```{r, fig.height = 5, echo = FALSE}
mod2_aug <- augment(mod2, type.predict = "response")

ggplot(mod2_aug, aes(x = temp, y = burst)) +
  geom_point(alpha = 0.4) +
  geom_line(aes(x = temp, y = .fitted), 
            col = "purple", size = 1.5) +
  labs(title = "Fitted Logistic mod2 for Pr(burst)")
```

Note that we're just connecting the predictions made for observed `temp` values with `geom_line`, so the appearance of the function isn't as smooth as the actual logistic regression model.

## Comparing the fits of `mod1` and `mod2`...

```{r}
#| fig-height: 5

p1 <- ggplot(orings1, aes(x = temp, y = burst)) +
    geom_jitter(height = 0.1) +
    geom_smooth(method = "lm", se = F, col = "red",
                formula = y ~ x) +
    labs(title = "Linear Probability mod1",
         y = "Burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")


p2 <- ggplot(mod2_aug, aes(x = temp, y = burst)) +
    geom_jitter(height = 0.1) +
    geom_line(aes(x = temp, y = .fitted), 
            col = "purple", size = 1.5) +
    labs(title = "Logistic Regression mod2",
         y = "Burst? (1 = yes, 0 = no)", 
         x = "Launch Temp (F)")

p1 + p2
```

## Could we try exponentiating the `mod2` coefficients?

How can we interpret the coefficients of the model?

$$
logit(burst) = log(odds(burst)) = 15.043 - 0.232 temp
$$

Exponentiating the coefficients is helpful...

```{r}
#| echo: true
exp(-0.232)
```

Suppose Launch A's temperature was one degree higher than Launch B's.

- The **odds** of Launch A having a burst are 0.793 times as large as they are for Launch B.
- Odds Ratio estimate comparing two launches whose `temp` differs by 1 degree is 0.793

## Exponentiated and tidied slope of `temp` (`mod2`)

```{r}
#| echo: true
tidy(mod2, exponentiate = TRUE, conf.int = TRUE) |>
    filter(term == "temp") |>
    select(term, estimate, std.error, conf.low, conf.high) |>
    kable(digits = 3)
```

- What would it mean if the Odds Ratio for `temp` was 1?
- How about an odds ratio that was greater than 1?

# A logistic regression model, fit with `lrm()` from **rms**

## Fitting the model again

```{r}
#| echo: true
d <- datadist(orings1)
options(datadist = "d")

mod3 <- lrm(burst ~ temp, data = orings1, x = TRUE, y = TRUE)
```

as compared to

```{r}
#| echo: true
mod2 <- glm(burst ~ temp, data = orings1,
            family =binomial(link ="logit"))
```

will fit the same model.

## `mod3` Results

![](c07/figures/fig1.png)

## `summary(mod3)` Results

![](c07/figures/fig2.png)

### Effects Plot

```{r}
#| echo: true
#| fig-height: 3

plot(summary(mod3))
```

## Predictions from `mod3`

```{r}
#| echo: true

newdat <- tibble(temp = c(50, 60, 70, 80))

## predictions on the log odds scale
predict(mod3, newdata = newdat)

## predictions on the probability scale
predict(mod3, newdata = newdat, type = c("fitted"))
```


## Plot in-sample predictions on log-odds scale

```{r}
#| echo: true
#| fig-height: 5

ggplot(Predict(mod3))
```

## Plot in-sample predictions on probability scale

```{r}
#| echo: true
#| fig-height: 5
ggplot(Predict(mod3, fun = plogis)) +
  labs(y = "Predicted Pr(burst) from mod3",
       title = "mod3 with the orings1 data")
```

## Nomogram for `mod3`

```{r}
#| echo: true
#| fig-height: 5
plot(nomogram(mod3, fun = plogis, funlabel = "Pr(burst)"), 
     lplabel="log odds (burst)")
```

## Regression on a Binary Outcome

**Linear Probability Model** (a linear model)

```
lm(event ~ predictor1 + predictor2 + ..., data = tibblename)
```

- Pr(event) is linear in the predictors

**Logistic Regression Model** (generalized linear model)

```
glm(event ~ pred1 + pred2 + ..., data = tibblename,
            family = binomial(link = "logit"))
or 

dd <- datadist(tibblename); options(datadist = "dd")
lrm(event ~ pred1 + pred2 + ..., data = tibblename, 
             x = TRUE, y = TRUE)
```

- Logistic Regression forces a prediction in (0, 1)
- log(odds(event)) is linear in the predictors

## The logistic regression model

$$
logit(event) = log\left( \frac{Pr(event)}{1 - Pr(event)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k
$$

$$
odds(event) = \frac{Pr(event)}{1 - Pr(event)}
$$

$$
Pr(event) = \frac{odds(event)}{odds(event) + 1}
$$

$$
Pr(event) = \frac{exp(logit(event))}{1 + exp(logit(event))}
$$ 

## Next Time

- Binary regression models with multiple predictors
- Assessing the quality of fit for a logistic model

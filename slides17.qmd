---
title: "432 Class 17"
author: "<https://thomaselove.github.io/432-2023/>"
date: "2023-03-07"
date-format: iso
format: 
  beamer:
    theme: Madrid
    colortheme: lily
    fig-align: center
---

## Today's Topic

**Regression Models for Ordered Multi-Categorical Outcomes**

- Applying to Graduate School: A First Example
- Proportional Odds Logistic Regression Models
  - Using `polr`
  - Using `lrm`
- Understanding and Interpreting the Model
- Testing the Proportional Odds Assumption
- Picturing the Model Fit
      
Chapter 27 of the Course Notes describes this material.

## Setup

```{r}
#| echo: true
#| warning: false
#| message: false

knitr::opts_chunk$set(comment=NA)
options(width = 60)

library(janitor)
library(GGally)
library(scales)
library(knitr)
library(rms)
library(MASS)
library(nnet)
library(tidyverse)

theme_set(theme_bw())
```

# Applying to Graduate School

## These are **simulated** data

This is a simulated data set of 530 students, looking at factors that influence the decision of whether to apply to graduate school. 

College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Hence, our outcome variable has three categories. Data on parental educational status, whether the undergraduate institution is public or private, and current GPA is also collected. The researchers have reason to believe that the "distances" between these three points are not equal. For example, the "distance" between "unlikely" and "somewhat likely" may be shorter than the distance between "somewhat likely" and "very likely".

```{r}
#| echo: true

gradschool <- 
  read_csv("c17/data/gradschool_new.csv",
           show_col_types = FALSE) |>
  type.convert(as.is = FALSE) |>
  clean_names()
```

## The `gradschool` data and my **Source**

The **gradschool** example is adapted from [\textcolor{blue}{this UCLA site}](http://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/). 

- There, they look at 400 students. 
- I simulated a new data set containing 530 students.

Variable | Description
---------: | ----------------------------------------------
`student` | subject identifying code (A001 - A530)
`apply`   | 3-level ordered outcome: "unlikely", "somewhat likely" and "very likely" to apply
`pared`   | 1 = at least one parent has a graduate degree, else 0
`public`  | 1 = undergraduate institution is public, else 0
`gpa`     | student's undergraduate grade point average (max 4.00)

## Ensuring that our outcome is an ordered factor

```{r}
#| echo: true

gradschool <- gradschool |>
    mutate(apply = fct_relevel(apply, "unlikely", 
                        "somewhat likely", "very likely"),
           apply = factor(apply, ordered = TRUE))

is.ordered(gradschool$apply)
```

## The `gradschool` tibble

```{r}
#| echo: true
gradschool
```

## Numerical Description of `gradschool` data

![]("c17/figures/fig1.png"){height=80%}

## Scatterplot Matrix (run with `#| message: FALSE`)

```{r}
#| echo: true
#| message: FALSE
#| fig.height: 5
ggpairs(gradschool |> select(gpa, pared, public, apply))
```


## Bar Chart of `apply` classifications with %s (code)

```{r}
#| echo: true
#| eval: false
ggplot(gradschool, aes(x = apply, fill = apply)) + 
    geom_bar(aes(y = 
        (after_stat(count)/sum(after_stat(count))))) +
    geom_text(aes(y = 
        (after_stat(count))/sum(after_stat(count)), 
          label = scales::percent((after_stat(count)) / 
                            sum(after_stat(count)))),
              stat = "count", vjust = 1.5, 
              color = "white", size = 5) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none") + 
    labs(y = "Percentage")
```

## Bar Chart of `apply` classifications with %s (result)

```{r}
ggplot(gradschool, aes(x = apply, fill = apply)) + 
    geom_bar(aes(y = 
        (after_stat(count)/sum(after_stat(count))))) +
    geom_text(aes(y = 
        (after_stat(count))/sum(after_stat(count)), 
          label = scales::percent((after_stat(count)) / 
                            sum(after_stat(count)))),
              stat = "count", vjust = 1.5, 
              color = "white", size = 5) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none") + 
    labs(y = "Percentage")
```


## Data (besides `gpa`) as Cross-Tabulation

```{r}
#| echo: true
ftable(xtabs(~ public + apply + pared, data = gradschool))
```

## `apply` percentages by `public`, `pared` (code)

```{r}
#| echo: true
#| eval: false
ggplot(gradschool, aes(x = apply, fill = apply)) + 
    geom_bar() +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none") + 
    facet_grid(pared ~ public, labeller = "label_both")
```


## `apply` percentages by `public`, `pared`

```{r}
ggplot(gradschool, aes(x = apply, fill = apply)) + 
    geom_bar() +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none") + 
    facet_grid(pared ~ public, labeller = "label_both")
```

## Breakdown of `gpa` by `apply` (code)

```{r}
#| echo: true
#| eval: false
ggplot(gradschool, aes(x = apply, y = gpa, fill = apply)) + 
    geom_violin(trim = TRUE) +
    geom_boxplot(col = "white", width = 0.2) +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none")
```

## Breakdown of `gpa` by `apply`

```{r}
ggplot(gradschool, aes(x = apply, y = gpa, fill = apply)) + 
    geom_violin(trim = TRUE) +
    geom_boxplot(col = "white", width = 0.2) +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none")
```

## `gpa` by three other variables (code)

```{r}
#| echo: true
#| eval: false

ggplot(gradschool, aes(x = apply, y = gpa)) +
    geom_boxplot(aes(fill = apply), size = .75) +
    geom_jitter(alpha = .25) +
    facet_grid(pared ~ public, margins = TRUE, 
               labeller = "label_both") +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none") +
    theme(axis.text.x = 
            element_text(angle = 45, hjust = 1, vjust = 1))
```

## `gpa` by three other variables

```{r}
ggplot(gradschool, aes(x = apply, y = gpa)) +
    geom_boxplot(aes(fill = apply), size = .75) +
    geom_jitter(alpha = .25) +
    facet_grid(pared ~ public, margins = TRUE, 
               labeller = "label_both") +
    scale_fill_brewer(palette = "Set1") +
    guides(fill = "none") +
    theme(axis.text.x = 
            element_text(angle = 45, hjust = 1, vjust = 1))
```



# Proportional Odds Logit Model via `polr`

## Fitting the POLR model with `MASS::polr`

We use the `polr` function from the `MASS` package:

```{r}
#| echo: true

mod_p1 <- polr(apply ~ pared + public + gpa, 
          data = gradschool, Hess=TRUE)
```

The `polr` name comes from proportional odds logistic regression, highlighting a key assumption of this model. 

`polr` uses the standard formula interface in R for specifying a regression model with outcome followed by predictors. We also specify `Hess=TRUE` to have the model return the observed information matrix from optimization (called the Hessian) which is used to get standard errors.

## Obtaining Predicted Probabilities from `mod_p1`

To start we'll obtain predicted probabilities, which are usually the best way to understand the model.

For example, we can vary `gpa` for each level of `pared` and `public` and calculate the model's estimated probability of being in each category of `apply`. 

First, create a new tibble of values to use for prediction.

```{r}
#| echo: true

newdat <- tibble(
  pared = rep(0:1, 200),
  public = rep(0:1, each = 200),
  gpa = rep(seq(from = 1.9, to = 4, length.out = 100), 4))
```

## Obtaining Predicted Probabilities from `mod_p1`

Now, make predictions using model `mod_p1`:

```{r}
#| echo: true

newdat_p1 <- cbind(newdat, 
                 predict(mod_p1, newdat, type = "probs"))
head(newdat_p1) |> kable(digits = 3)
```

## Reshape data

Now, we reshape the data with `pivot_longer`:

```{r}
#| echo: true

newdat_long <- 
  pivot_longer(newdat_p1, 
               cols = c("unlikely":"very likely"),
               names_to = "level",
               values_to = "probability") |>
  mutate(level = fct_relevel(level, "unlikely",
                             "somewhat likely"))
```

Result on next slide...

## The `newdat_long` data

```{r}
newdat_long
```


## Plot the prediction results... (code)

```{r}
#| echo: true
#| eval: false

ggplot(newdat_long, aes(x = gpa, y = probability, 
                        color = level)) +
    geom_line(size = 1.5) + 
    scale_color_brewer(palette = "Set1") +
    facet_grid(pared ~ public, labeller="label_both")
```


## Plot the prediction results...

```{r}
#| fig.height: 5.5

ggplot(newdat_long, aes(x = gpa, y = probability, 
                        color = level)) +
    geom_line(size = 1.5) + 
    scale_color_brewer(palette = "Set1") +
    facet_grid(pared ~ public, labeller="label_both")
```


## Cross-Tabulation of Predicted/Observed Classifications

Predictions in the rows, Observed in the columns

```{r}
#| echo: true

addmargins(table(predict(mod_p1), gradschool$apply))
```

We only predict one subject to be in the "very likely" group by modal prediction.

## Describing the Proportional Odds Logistic Model

Our outcome, `apply`, has three levels. Our model has two logit equations: 

- one estimating the log odds that `apply` will be less than or equal to 1 (`apply` = "unlikely") 
- one estimating the log odds that `apply` $\leq$ 2 (`apply` = "unlikely" or "somewhat likely")

That's all we need to estimate the three categories, since Pr(`apply` $\leq$ 3) = 1, because "very likely" is the maximum category for `apply`.

## Parameters of the POLR Model

- The parameters to be fit include two intercepts:
    - $\zeta_1$ will be the `unlikely|somewhat likely` parameter
    - $\zeta_2$ will be the `somewhat likely|very likely` parameter

We'll have a total of five free parameters when we add in the slopes ($\beta$) for `pared`, `public` and `gpa`.

- The two logistic equations that will be fit differ only by their intercepts.

## `summary(mod_p1)`

```{r}
summary(mod_p1)
```

## Understanding the Model

$$ 
logit[Pr(apply \leq 1)] = \zeta_1 - \beta_1 pared - \beta_2 public - \beta_3 gpa
$$

$$ 
logit[Pr(apply \leq 2)] = \zeta_2 - \beta_1 pared - \beta_2 public - \beta_3 gpa
$$

in general. In our setting, we have ...

## The `mod_p1` equations...

$$ 
logit[Pr(apply \leq unlikely)] = 
  3.87 - 1.15 pared - (-0.49) public - 1.14 gpa
$$
and
$$
logit[Pr(apply \leq somewhat)] = 5.94 - 1.15 pared - (-0.49) public - 1.14 gpa
$$

## `confint(mod_p1)`

Confidence intervals for the slope coefficients on the log odds scale can be estimated in the usual way.

```{r}
confint(mod_p1)
```

These CIs describe results in units of ordered log odds.

- For example, for a one unit increase in `gpa`, we expect a 1.14 increase in the expected value of `apply` (95% CI 0.78, 1.51) in the log odds scale, holding `pared` and `public` constant.
- This would be more straightforward if we exponentiated.

## Exponentiating the Coefficients

```{r}
#| echo: true

exp(coef(mod_p1))
exp(confint(mod_p1))
```

## Interpreting the Exponentiated Coefficients

Variable | Estimate | 95% CI
--------: | -------: | --------------:
`gpa` | 3.13 | (2.19, 4.53)
`public` | 0.61 | (0.39, 0.93)
`pared` | 3.17 | (2.07, 4.87)

- When a student's `gpa` increases by 1 unit, the odds of moving from "unlikely" applying to "somewhat likely" or "very likely" applying are multiplied by 3.13 (95% CI 2.19, 4.52), all else held constant. 
- For `public`, the odds of moving from a lower to higher `apply` status are multiplied by 0.61 (95% CI 0.39, 0.93) as we move from private to public, all else held constant.
- How about `pared`?

## Comparison to a Null Model

```{r}
#| echo: true

mod_p0 <- polr(apply ~ 1, data = gradschool)

anova(mod_p1, mod_p0)
```

## AIC and BIC are available, too

We could also compare model `mod_p1` to the null model `mod_p0` with AIC or BIC.

```{r}
#| echo: true

AIC(mod_p1, mod_p0)
BIC(mod_p1, mod_p0)
```

## Testing the Proportional Odds Assumption

One way to test the proportional odds assumption is to compare the fit of the proportional odds logistic regression to a model that does not make that assumption. A natural candidate is a **multinomial logit** model, which is typically used to model unordered multi-categorical outcomes, and fits a slope to each level of the `apply` outcome in this case, as opposed to the proportional odds logit, which fits only one slope across all levels.

Since the proportional odds logistic regression model is nested in the multinomial logit, we can perform a likelihood ratio test. To do this, we first fit the multinomial logit model, with the `multinom` function from the `nnet` package.

## Fitting the multinomial model

Again, the multinomial model is fit here using `multinom()` from the **nnet** package...

```{r}
#| echo: true
m1_multi <- multinom(apply ~ pared + public + gpa, 
                      data = gradschool)
```

## The multinomial model

```{r}
#| echo: true
m1_multi
```

## Comparing the Models

The multinomial logit fits two intercepts and six slopes, for a total of 8 estimated parameters. 

The proportional odds logit, as we've seen, fits two intercepts and three slopes, for a total of 5. The difference is 3, and we use that number in the sequence below to build our test of the proportional odds assumption.

## Testing the Proportional Odds Assumption

```{r}
#| echo: true
LL_1 <- logLik(mod_p1)
LL_1m <- logLik(m1_multi)
(G <- -2 * (LL_1[1] - LL_1m[1]))
pchisq(G, 3, lower.tail = FALSE)
```

The *p* value is 0.018, so it indicates that the proportional odds model fits less well than the more complex multinomial logit. 

## Comparing AIC and BIC

```{r}
#| echo: true
AIC(mod_p1)
AIC(m1_multi)

BIC(mod_p1)
BIC(m1_multi)
```

## What to do in light of these results...

- A non-significant *p* value here isn't always the best way to assess the proportional odds assumption, but it does provide some evidence of model adequacy.
- The stronger BIC (and only slightly worse AIC) for our POLR model relative to the multinomial gives us some conflicting advice.
    - One alternative would be to fit the multinomial model instead. 
    - Another would be to fit a check of residuals (see Frank Harrell's RMS text.)
    - Another would be to fit a different model for ordinal regression. Several are available (check out `orm` in the `rms` package, for instance.)

# Using `lrm` for Proportional Odds Logistic Regression

## Using `lrm` to work through this model

```{r}
#| echo: true
d <- datadist(gradschool)
options(datadist = "d")
mod <- lrm(apply ~ pared + public + gpa, 
           data = gradschool, x = T, y = T)
```

## `mod` output

![]("c17/figures/fig2.png"){width=90%}

## `summary(mod)`

```{r}
summary(mod)
```

## `plot(summary(mod))`

```{r}
plot(summary(mod))
```

## Coefficients in our equation

```{r}
#| echo: true

mod$coef
```

## Nomogram of `mod` (code)

```{r}
#| echo: true
#| eval: false
fun.1 <- function(x) 1 - plogis(x)
fun.3 <- function(x) 
    plogis(x - mod$coef[1] + mod$coef[2])

plot(nomogram(mod,
    fun=list('Prob Y = 1 (unlikely)' = fun.1, 
             'Prob Y = 3 (very likely)' = fun.3)))
```

## Nomogram of `mod` (result)

```{r}
fun.1 <- function(x) 1 - plogis(x)
fun.3 <- function(x) 
    plogis(x - mod$coef[1] + mod$coef[2])

plot(nomogram(mod,
    fun=list('Prob Y = 1 (unlikely)' = fun.1, 
             'Prob Y = 3 (very likely)' = fun.3)))
```

## `set.seed(432); validate(mod)`

```{r}
set.seed(432); validate(mod)
```

## More to come.

We'll share another example of ordinal logistic regression before we're done.


